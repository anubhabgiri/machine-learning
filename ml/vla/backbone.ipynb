{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs {'pixel_values': tensor([[[[-1.4329, -1.4329, -1.4329,  ..., -1.4329, -1.4329, -1.4329],\n",
      "          [-1.4329, -1.4329, -1.4329,  ..., -1.4329, -1.4329, -1.4329],\n",
      "          [-1.4329, -1.4329, -1.4329,  ..., -1.4329, -1.4329, -1.4329],\n",
      "          ...,\n",
      "          [ 2.0605,  2.0605,  2.0605,  ...,  2.0605,  2.0605,  2.0605],\n",
      "          [ 2.0605,  2.0605,  2.0605,  ...,  2.0605,  2.0605,  2.0605],\n",
      "          [ 2.0605,  2.0605,  2.0605,  ...,  2.0605,  2.0605,  2.0605]],\n",
      "\n",
      "         [[-1.3354, -1.3354, -1.3354,  ..., -1.3354, -1.3354, -1.3354],\n",
      "          [-1.3354, -1.3354, -1.3354,  ..., -1.3354, -1.3354, -1.3354],\n",
      "          [-1.3354, -1.3354, -1.3354,  ..., -1.3354, -1.3354, -1.3354],\n",
      "          ...,\n",
      "          [ 2.2010,  2.2010,  2.2010,  ...,  2.2010,  2.2010,  2.2010],\n",
      "          [ 2.2010,  2.2010,  2.2010,  ...,  2.2010,  2.2010,  2.2010],\n",
      "          [ 2.2010,  2.2010,  2.2010,  ...,  2.2010,  2.2010,  2.2010]],\n",
      "\n",
      "         [[-1.1073, -1.1073, -1.1073,  ..., -1.1073, -1.1073, -1.1073],\n",
      "          [-1.1073, -1.1073, -1.1073,  ..., -1.1073, -1.1073, -1.1073],\n",
      "          [-1.1073, -1.1073, -1.1073,  ..., -1.1073, -1.1073, -1.1073],\n",
      "          ...,\n",
      "          [ 2.3437,  2.3437,  2.3437,  ...,  2.3437,  2.3437,  2.3437],\n",
      "          [ 2.3437,  2.3437,  2.3437,  ...,  2.3437,  2.3437,  2.3437],\n",
      "          [ 2.3437,  2.3437,  2.3437,  ...,  2.3437,  2.3437,  2.3437]]]])}\n",
      "Shape of the feature vector: torch.Size([1, 257, 768])\n",
      "features tensor([[[-0.7701,  1.2928,  0.0685,  ..., -0.6475,  0.9421,  1.2417],\n",
      "         [-0.0187, -1.2202, -0.7044,  ..., -2.2026, -2.4774,  0.2517],\n",
      "         [ 0.2693,  2.4182, -0.4973,  ...,  0.3492, -3.7005,  0.3649],\n",
      "         ...,\n",
      "         [ 0.2586,  0.6654, -0.8376,  ...,  0.9047, -1.7900, -0.1635],\n",
      "         [ 0.1109, -0.2485, -0.1484,  ...,  1.0049, -0.5792,  0.9624],\n",
      "         [ 1.1552,  0.1653, -1.0976,  ...,  0.7416, -0.5684,  0.6595]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import Dinov2Model, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the pre-trained DINOv2 model and processor\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "\n",
    "# Step 2: Load an input image\n",
    "image = Image.open(\"linkedin_feed_sample.png\")  # Replace with your image path\n",
    "\n",
    "# Step 3: Preprocess the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"inputs {inputs}\")\n",
    "# Step 4: Extract features using DINOv2\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The extracted visual features\n",
    "features = outputs.last_hidden_state\n",
    "print(f\"Shape of the feature vector: {features.shape}\")\n",
    "print(f\"features {features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: )*������y*+y�����yW�D�Dyyr)rr�wDD)qr)yrDr�+rrLy++)r*+yr+Dr+rrry+yyrqyrry+r+++r�+yrk+ryy+rr+rrr�++rr��+++y$+++q�++yrr++00qq+qyr++yrrr0r0Z0rr0rry+��rr000+00+00r)rwrrr0|0000+0r))Cwryr000+0++00r)CC�Wrr0r0rrrrrrwCy�Lr�ryryrrrrr))y�Lyyr��r:y:r�D�)|Lr��y?|r+rr��D|r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r+r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Step 1: Define a simple MLP to map DINOv2 features to token IDs\n",
    "class FeatureToTokenMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeatureToTokenMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 2: Load your pre-trained DINOv2 features\n",
    "# Assuming features is the tensor extracted from DINOv2 (shape: [batch_size, tokens, feature_dim])\n",
    "# Example dimensions: (1, 197, 768) -> Taking 768 as feature_dim\n",
    "\n",
    "# Step 3: Initialize the MLP\n",
    "input_dim = features.shape[-1]  # 768 (feature dimension of DINOv2)\n",
    "hidden_dim = 512  # Hidden layer size\n",
    "output_dim = 128  # Mapping to a smaller dimension that represents token IDs\n",
    "mlp = FeatureToTokenMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Step 4: Pass the DINOv2 features through the MLP\n",
    "# Flatten features and process them\n",
    "flattened_features = features.view(-1, input_dim)  # Shape: [batch_size * tokens, input_dim]\n",
    "mlp_output = mlp(flattened_features)\n",
    "\n",
    "# Step 5: Convert MLP output to token-like representations (Simulating token IDs)\n",
    "# For simplicity, we're mapping to a token range for GPT-2 (50257 vocab size)\n",
    "vocab_size = 50257  # GPT-2 tokenizer vocabulary size\n",
    "mlp_token_ids = torch.argmax(mlp_output, dim=-1) % vocab_size  # Shape: [batch_size * tokens]\n",
    "\n",
    "# Step 6: Token IDs are now generated, and we can pass them into GPT-2 for text generation\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# model_name = \"MiniGPT-4\"  # You can look for a smaller version if available\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Reshape token IDs back to (batch_size, tokens)\n",
    "mlp_token_ids = mlp_token_ids.view(1, -1)  # Batch of token sequences\n",
    "\n",
    "attention_mask = torch.ones(mlp_token_ids.shape, dtype=torch.long)\n",
    "\n",
    "# Step 7: Generate text using GPT-2 from token-like IDs\n",
    "# Note: Simulating input token text based on MLP token output\n",
    "generated_output = model.generate(input_ids=mlp_token_ids, attention_mask=attention_mask, max_length=300)\n",
    "\n",
    "# Step 8: Decode the generated output to get human-readable text\n",
    "generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(\"Generated response:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated feature text: Feature_0: [-0.7701189517974854, 1.2928109169006348, 0.06849299371242523, -0.26569458842277527, -1.7128863334655762] Feature_1: [-0.018731415271759033, -1.2202171087265015, -0.7043544054031372, -1.6775083541870117, -1.361791968345642] Feature_2: [0.2693025469779968, 2.41815185546875, -0.49726587533950806, -2.247089147567749, 0.6631515622138977] Feature_3: [-0.24857012927532196, 2.2584683895111084, -1.543166995048523, -2.7039871215820312, 1.346882939338684] Feature_4: [0.4978777766227722, 2.021416187286377, 0.002517223358154297, -1.9776861667633057, -0.059578314423561096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: Feature_0: [-0.7701189517974854, 1.2928109169006348, 0.06849299371242523, -0.26569458842277527, -1.7128863334655762] Feature_1: [-0.018731415271759033, -1.2202171087265015, -0.7043544054031372, -1.6775083541870117, -1.361791968345642] Feature_2: [0.2693025469779968, 2.41815185546875, -0.49726587533950806, -2.247089147567749, 0.6631515622138977] Feature_3: [-0.24857012927532196, 2.2584683895111084, -1.543166995048523, -2.7039871215820312, 1.346882939338684] Feature_4: [0.4978777766227722, 2.021416187286377, 0.002517223358154297, -1.9776861667633057, -0.059578314423561096] Feature_5: [0.4978777766\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Assuming you have already extracted the features using DINOv2\n",
    "\n",
    "# Step 1: Convert features to text (a simple method to just convert tensor values to text)\n",
    "# Here we're converting the first 5 feature vectors into a string (you can customize this)\n",
    "feature_text = \" \".join([f\"Feature_{i}: {features[0, i, :5].tolist()}\" for i in range(5)])\n",
    "print(\"Generated feature text:\", feature_text)\n",
    "\n",
    "# Step 2: Load a small LLM (GPT2 or a smaller variant like distilgpt2)\n",
    "model_name = \"distilgpt2\"  # This is a lightweight model suitable for local machines\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Step 3: Tokenize the text and pass it to the model\n",
    "inputs = tokenizer(feature_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n",
    "\n",
    "# Step 4: Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated response:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not work because distilgp2 was not able to generate any meaningful response from the input. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
